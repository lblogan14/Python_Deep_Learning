{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch7_rnn_lang_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/Python_Deep_Learning/blob/master/ch7_rnn_lang_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvB72tS3rOWY",
        "colab_type": "text"
      },
      "source": [
        "#Recurrent Neural Networks\n",
        "RNNs help deal with sequences of variable length by defining a recurrence relation over these sequences. \n",
        "\n",
        "An RNN is defined as a recurrence relation:\n",
        "$$s_t=f(s_{t-1},x_t)$$\n",
        "where $f$ is a differentiable function, $s$ is a vector of values called internal network state (at step $t$), and $x_t$ is the network input at step $t$. $s_t$ is a function of both the current input as well as the previous state $s_{t-1}$. The recurrence relation defines how the state evolves step by step over the sequence via a feedback loop over previous states:\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/rnn.PNG?raw=true)\n",
        "\n",
        "RNN has three sets of parameters (or weights):\n",
        "* $U$ transforms the input $x_t$ to the state $s_t$\n",
        "* $W$ transforms the previous state $s_{t-1}$ to the output $s_t$\n",
        "* $V$ maps the newly computed internal state $s_t$ to the output $y_t$\n",
        "\n",
        "The internal state and the network output are defined as:\n",
        "$$s_t=f(s_{t-1}*W+x_t*U)$$\n",
        "$$y_t=s_t*V$$\n",
        "where $f$ is the non-linear activation function.\n",
        "\n",
        "In a word-level language model, the input $x$ is a sequence of words encoded in input vectors $(x_1...x_t...)$. The state $s$ will be a sequence of state vectors $(s_1...s_t...)$. The output $y$ will be a sequence of probability vectors $(y_1...y_t...)$ of the next words in sequence.\n",
        "\n",
        "In a RNN, each state is dependent on all previous computations via this\n",
        "recurrence relation.\n",
        "\n",
        "A **stacked RNN** is form if multiple RNNs are stacked, as shown below. The cell state $s_t^l$ of a RNN cell at level $l$ at time $t$ will take the output $y_t^{l-1}$ of the RNN cell from level $l-1$ and previous cell state $s_{t-1}^l$ of the cell at the same level $l$ as the input:\n",
        "$$s_t^l = f(s_{t-1}^l, y_t^{l-1})$$\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/stacked_rnn.PNG?raw=true)\n",
        "\n",
        "The following are the possible applications of RNN:\n",
        "* **One-to-one**: Non-sequential processing. Example: image classification\n",
        "* **One-to-many**: This processing generates a sequence based on a single input, for example, caption generation from an image\n",
        "* **Many-to-one**: This processing outputs a single result based on a sequence, for example, sentiment classification from text.\n",
        "* **Many-to-many indirect**: A sequence is encoded into a state vector, after which this state vector is decoded into a new sequence, for example, language translation\n",
        "* **Many-to-many direct**: This outputs a result for each intput step, for example, frame phoneme labeling in speech recognition.\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/rnn_combination.PNG?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2C8xg2dxlDh",
        "colab_type": "text"
      },
      "source": [
        "##RNN Implementation\n",
        "Use RNN to train with a toy example, counting ones in q sequence, which is a \"many-to-one\" relationship. The RNN is shown below:\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/basic_rnn.PNG?raw=true)\n",
        "\n",
        "The network will have only two parameters: an input weight $U$ and a recurrence weight $W$. The output weight $V$ is set to 1 so as to read out the last state as the output $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lde0KOja1p6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytJhYhkZ1q8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The first dimension represents the mini-batch\n",
        "x = np.array([[0,0,0,0,1,0,1,0,1,0]])\n",
        "y = np.array([3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjtMMJid3vtS",
        "colab_type": "text"
      },
      "source": [
        "Define the recurrence realtion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX0nBUXa3yIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def step(s, x, U, W):\n",
        "    return x * U + s * W"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRhkhrVN5BWf",
        "colab_type": "text"
      },
      "source": [
        "All the inputs are scalar values. If U=1, then wheneve input is received, its full value is obtained. If W=1, then the accumulated value would never decay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr4vpaw_5Tcq",
        "colab_type": "text"
      },
      "source": [
        "###Backpropagation through time\n",
        "The main difference between regular backpropagation and backpropagation through time is that the recurrent network is unfolded through time for a certain number of time steps as shown before. One hidden layer of that network represents one step through time. THe only differences are that each layer has multiple inputs: the previous state $s_{t-1}$ and the current input $x_t$. The parameters $U$ and $W$ are shared between all hidden layers.\n",
        "\n",
        "The forward pass unwraps the RNN along the sequence and builds a stack of states for each step. The following is an implementation of the forward pass, which returns the activation $s$ for each recurrent step and each sample in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhLL4KsK8L4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, U, W):\n",
        "    # Number of samples in the mini-batch\n",
        "    number_of_samples = len(x)\n",
        "\n",
        "    # Length of each sample\n",
        "    sequence_length = len(x[0])\n",
        "\n",
        "    # Initialize the state activation for each sample along the sequence\n",
        "    s = np.zeros((number_of_samples, sequence_length + 1))\n",
        "\n",
        "    # Update the states over the sequence\n",
        "    for t in range(0, sequence_length):\n",
        "        s[:, t+1] = step(s[:,t], x[:,t], U, W) # step function\n",
        "\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPSpIytg9LqX",
        "colab_type": "text"
      },
      "source": [
        "Since the unfolded RNN is equivalent to a regular feedforward network, use chain rule in backpropagation.\n",
        "\n",
        "Because the weights $W$ and $U$ are shared across the layers, the error derivatives are accumulated for each recurrent step and then the weightes are update in the end with the accumulated value.\n",
        "\n",
        "The recurrence relation to propagate the gradient through the network can be written as\n",
        "$$\\frac{\\partial J}{\\partial s_{t-1}}=\\frac{\\partial J}{\\partial s_t}\\frac{\\partial s_t}{\\partial s_{t-1}}=\\frac{\\partial J}{\\partial s_t}W$$\n",
        "where $J$ is the loss function.\n",
        "\n",
        "The gradients of the parameters are accumulated as follows:\n",
        "$$\\frac{\\partial J}{\\partial U}=\\sum_{t=0}^{n}\\frac{\\partial J}{\\partial s_{t}}x_t$$\n",
        "$$\\frac{\\partial J}{\\partial W}=\\sum_{t=0}^{n}\\frac{\\partial J}{\\partial s_{t}}s_{t-1}$$\n",
        "\n",
        "The following is an implementation of the backward pass:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LjSiva4_Pro",
        "colab_type": "text"
      },
      "source": [
        "1. Accumulate the gradients $U$ and $W$ in `gU` and `gW`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woqqkDRK8_z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(x, s, y, W):\n",
        "    sequence_length = len(x[0])\n",
        "\n",
        "    # The network output is just the last activation of sequence\n",
        "    s_t = s[:, -1]\n",
        "\n",
        "    # Compute the gradient of the output w.r.t. MSE cost function at final state\n",
        "    gS = 2 * (s_t - y)\n",
        "\n",
        "    # Set the gradient accumulations to 0\n",
        "    gU, gW = 0, 0\n",
        "\n",
        "    # Accumulate gradients backwards\n",
        "    for k in range(sequence_length, 0, -1):\n",
        "        # compute the parameter gradients and accumulate the results\n",
        "        gU += np.sum(gS * x[:, k-1])\n",
        "        gW += np.sum(gS * s[:, k-1])\n",
        "\n",
        "        # compute the gradient at the output of the previous layer\n",
        "        gS = gS * W\n",
        "\n",
        "    return gU, gW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMMT7lKv_YQ6",
        "colab_type": "text"
      },
      "source": [
        "2. Use gradient descent to optimize the network with MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaizVbS0_c9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(x, y, epochs, learning_rate = 0.0005):\n",
        "    '''Train the network'''\n",
        "\n",
        "    # Set initial parameters\n",
        "    weights = (-2, 0) # (U, W)\n",
        "\n",
        "    # Accumulate the losses and their respective weights\n",
        "    losses = list()\n",
        "    weights_u = list()\n",
        "    weights_w = list()\n",
        "\n",
        "    # Perform iterative gradient descent\n",
        "    for i in range(epochs):\n",
        "        # Perform forward and backward pass to get the gradients\n",
        "        s = forward(x, weights[0], weights[1])\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = (y[0] - s[-1, -1])**2\n",
        "\n",
        "        # Store the loss and weights values for later display\n",
        "        losses.append(loss)\n",
        "\n",
        "        weights_u.append(weights[0])\n",
        "        weights_w.append(weights[1])\n",
        "\n",
        "        gradients = backward(x, s, y, weights[1])\n",
        "\n",
        "        # Update each parameter `p` by p = p - (gradient * learning_rate)\n",
        "        # `gp` is the gradient of parameter `p`\n",
        "        weights = tuple((p - gp*learning_rate) for p, gp in zip(weights, gradients))\n",
        "\n",
        "    print(weights)\n",
        "\n",
        "    return np.array(losses), np.array(weights_u), np.array(weights_w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFOIcDSHK5Du",
        "colab_type": "text"
      },
      "source": [
        "3. Implement the `plot_training` function to display the weights and the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvFIRM82AmUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_training(losses, weights_u, weights_w):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # remove nan and inf values\n",
        "    losses = losses[~np.isnan(losses)][:-1]\n",
        "    weights_u = weights_u[~np.isnan(weights_u)][:-1]\n",
        "    weights_w = weights_w[~np.isnan(weights_w)][:-1]\n",
        "\n",
        "    # plot the weights U and W\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3.4))\n",
        "\n",
        "    ax1.set_ylim(-3, 2)\n",
        "    ax1.set_xlabel('epochs')\n",
        "    ax1.plot(weights_w, label='W', color='red', linestyle='--')\n",
        "    ax1.plot(weights_u, label='U', color='blue', linestyle=':')\n",
        "    ax1.legend(loc='upper left')\n",
        "\n",
        "    # instantiate a second axis that shares the same x-axis\n",
        "    # plot the loss on the second axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # uncomment to plot exploding gradients\n",
        "    ax2.set_ylim(-3, 200)\n",
        "    ax2.plot(losses, label='Loss', color='green')\n",
        "    ax2.tick_params(axis='y', labelcolor='green')\n",
        "    ax2.legend(loc='upper right')\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxXh9E95MvbO",
        "colab_type": "text"
      },
      "source": [
        "4. Run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJVFQtrmMuvT",
        "colab_type": "code",
        "outputId": "0e545ccb-e6db-49a3-e5d5-af8afe15ad09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "losses, weights_u, weights_w = train(x, y, epochs=150)\n",
        "plot_training(losses, weights_u, weights_w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(-2.088692479228279, -0.7529413189879612)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADtCAYAAACBOK/+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x244.8 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bk6VgPFNOD8",
        "colab_type": "text"
      },
      "source": [
        "###Vanishing and exploding gradients\n",
        "The preceding code has an issue. Try with a longer sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zks-MgmoM_3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]])\n",
        "y = np.array([12])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ7KOKKDNdxu",
        "colab_type": "code",
        "outputId": "199443da-6eea-4cc9-d629-954787dd4e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "losses, weights_u, weights_w = train(x, y, epochs=150)\n",
        "plot_training(losses, weights_u, weights_w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in multiply\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(nan, nan)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADtCAYAAACBOK/+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTJhYADbGmDCxAGyMMWHy/wEfYxLDg4hBCgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x244.8 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V43lyewONnRp",
        "colab_type": "text"
      },
      "source": [
        "The reason for these warnings is that the final parameters $U$ and $W$ end up as **Not a Number (NaN)**, as shown in the graph.\n",
        "\n",
        "The weights slowly move toward the optimum and the loss decreases until it overshoots at epoch 23. What happens is that the cost surface trained on is highly unstable. This is known as exploding gradients.\n",
        "\n",
        "There is also the vanishing gradient problem. The gradient decays exponentially over the number of steps to a point where it becomes extremely smaller in the earlier states. This problem is harder to detect because the training will still work and the network will produce valid outputs (unlike with exploding gradients). It just won't be able to learn long-term dependencies.\n",
        "\n",
        "* Depending on the sequence's length, an unfolded RNN can be much deeper compared to a regular network.\n",
        "* The weights W are shared across all steps, which means that the recurrence relation that propagates the gradient backward through time forms a geometric sequence:\n",
        "$$\\frac{\\partial s_t}{\\partial s_{t-m}}=W^m$$\n",
        "In this simple linear RNN, the gradient grows exponentially if $|W|>1$ (exploding gradient)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n4ZXO_rM3dt",
        "colab_type": "text"
      },
      "source": [
        "##Long Short-Term Memory\n",
        "LSTMs can handle long-term dependencies due to a specially crafted memory cell. The key idea of LSTM is the cell state (in addition to the hidden RNN state), where the information can only be explicitly written in or removed so that the state stays constant if there is no outside interference.\n",
        "\n",
        "A typical LSTM is composed of three gates: a forget gate, an input gate, and an output gate.\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/lstm_cell.PNG?raw=true)\n",
        "\n",
        "* $x_t, c_t, h_t$ are the LSTM input, cell memory state, and output (or hidden state) in moment $t$. $c_t'$ is the candidate cell state. The input $x_t$ and the previous cell output $h_{t-1}$ are connected to each gate and the candidate cell vector with sets of weights $W$ and $U$, respectively.\n",
        "* $c_t$ is the cell state in moment $t$.\n",
        "* $f_t, i_t, o_t$ are the forget, input, and output gates of the LSTM cell.\n",
        "\n",
        "The forget gate bases its decision on the output of the previous cell $h_{t-1}$ and the current input $x_t$:\n",
        "$$f_t = \\sigma(W_f x_t + U_f h_{t-1})$$\n",
        "It applies element-wise logistic functions on each element of the previous cell's vector $c_{t-1}$. An output of 0 erases a specific $c_{t-1}$ cell block completely and an output of 1 allows the information in that cell block to pass through.\n",
        "\n",
        "The input gate decides what new information is going to be added to the memory cell. It bases its decision on $h_{t-1}$ and $x_t$. An output of 0 means that no information is added to that cell block's memory.\n",
        "$$i_t = \\sigma(W_i x_t + U_i h_{t-1})$$\n",
        "\n",
        "The candidate input to be added, $c_t'$, is based on the previous output $h_{t-1}$ and the current input $x_t$:\n",
        "$$c_t' = \\tanh(W_c x_t + U_c h_{t-1})$$\n",
        "\n",
        "The forget and input gates decide the new cell state by choosing which parts of the new and the old state to include:\n",
        "$$c_t = f_t * c_{t-1} \\oplus i_t * c_t'$$\n",
        "The output gate decides what the total cell output is going to be. It takes $h_{t-1}$ and $x_t$ as inputs and outputs 0 or 1 (via the logistic function) for each block of the cell's memory. An output of 0 means that the block doesn't output any information, while an output of 1 means that the block can pass through as a cell's output.\n",
        "$$o_t = \\sigma(W_o x_t + U_o h_{t-1})$$\n",
        "The LSTM cell output:\n",
        "$$h_t = o_t * \\tanh(c_t)$$\n",
        "\n",
        "To protect from vanishing gradients, the cell state is copied identically from step to step if the forget gate is 1 and the input gate is 0. Only the\n",
        "forget gate can completely erase the cell's memory. As a result, memory can remain unchanged over a long period of time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPx8S5irc8iK",
        "colab_type": "text"
      },
      "source": [
        "##Gated Recurrent Units\n",
        "A GRU unit does similar or better than a LSTM but with fewer parameters and operations.\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/gru_cell.PNG?raw=true)\n",
        "\n",
        "The GRU cell has two gates:\n",
        "* An update gate $z_t$, which is a combination of the input and forget LSTM gates. It decides what information to discard and what new information to include in its place, based on the network input $x_t$ and the previous cell hidden state $h_{t-1}$.\n",
        "$$z_t = \\sigma(W_z x_t + U_z h_{t-1})$$\n",
        "* A reset gate $r_t$, which uses the previous cell state $h_{t-1}$ and the network input $x_t$ to decide how much of the previous state to pass through:\n",
        "$$r_t = \\sigma(W_r x_t + U_r h_{t-1})$$\n",
        "\n",
        "The candidate state $h_t'$:\n",
        "$$h_t'=\\tanh(W x_t + U(r_t*h_{t-1})$$\n",
        "The GRU output $h_t$ at time $t$:\n",
        "$$h_t = (1-z_t)*h_{t-1} \\oplus z_t* h_t'$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ-rMQkBeWWO",
        "colab_type": "text"
      },
      "source": [
        "#Language Modeling\n",
        "Language modeling is the task of computing the probability of a sequence of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJn-QN_rdVZY",
        "colab_type": "text"
      },
      "source": [
        "##Word-based Models\n",
        "A word-based language model defines a probability distribution over sequences of words. Given a sequence of words of length $m$, it assigns a probability $P(w_1,...,w_m)$ to the full sequence of words. These probabilities can be used to\n",
        "* To estimate the likelihood of different phrases in natural language processing applications.\n",
        "* To compute the likelihood of a given word to follow a sequence of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggrYX2IHeNg4",
        "colab_type": "text"
      },
      "source": [
        "###N-grams\n",
        "The inference of the probability of a long sequence is typically infeasible. Calculating the joint probability of $P(w_1,...,w_m)$:\n",
        "$$P(w_1,...,w_m)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_m|w_1,...,w_{m-1})$$\n",
        "The n-grams is modeled the joint probabilities of combinations of $n$ sequential words.\n",
        "\n",
        "Given a huge corpus of text, the probabilities of the last word of each n-gram, given the previous $n-1$ words:\n",
        "* **1-gram**: $P(word) = \\frac{count(word)}{\\mbox{total number of words in corpus}}$\n",
        "* **2-gram**: $P(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)}{count(w_{i-1})}$\n",
        "* **N-gram**: $P(w_{n+i}|w_n,...,w_{n+i-1})=\\frac{count(w_n,...,w_{n+i-1},w_{n+i})}{count(w_n,...,w_{n+i-1})}$\n",
        "\n",
        "The $i$th word is only dependent on the previous $n-1$ words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XixoN80HiSy_",
        "colab_type": "text"
      },
      "source": [
        "###Neural language models\n",
        "One way to overcome the curse of dimensionality is by learning a lower dimensional, distributed representation of the words. This distributed representation is created by learning an embedding function that transforms the space of words into a lower dimensional space of word embeddings, as follows:\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/embedding.PNG?raw=true)\n",
        "\n",
        "Words from the vocabulary with size $V$ are transformed into one-hot encoding vectors of size $V$ (each word is encoded uniquely). Then, the embedding function transforms this $V$-dimensional space into a distributed representation of size $D$.\n",
        "\n",
        "The idea is that the embedding function learns semantic information about the words. It associates each word in the vocabulary with a continuous-valued vector representation, that is, the word embedding. Each word corresponds to a point in this embedding space, and different dimensions correspond to the grammatical or semantic properties of these\n",
        "words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzXVF-E7pEJD",
        "colab_type": "text"
      },
      "source": [
        "####Neural probabilistic language model\n",
        "Given a sequence of $n-1$ words $(w_{t-n+1},...,w_{t-1})$, the fully-connected network tries to output the probability distribution of the next word $w_t$.\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/neural_prob_lang_model.PNG?raw=true)\n",
        "\n",
        "1. The embedding layer takes the one-hot representation of the word $w_i$ and\n",
        "transforms it into the word's embedding vector by multiplying it with the\n",
        "embedding matrix $C$. The embedding matrix $C$ is shared over the words, so all words use the same embedding function. $C$ is represented by a $V*D$ matrix, where $V$ is the size of the vocabulary and $D$ is the size of the embedding.\n",
        "2. The resulting embeddings are concatenated and serve as an input to the hidden layer, which uses `tanh` activation. The output of the hidden layer is thus $z=\\tanh\\left(H\\cdot (concat(C(w_{t-n+1}),...,C(w_{t-1})) + d)\\right)$, where $H$ is the embedding-to-hidden layer weights and $d$ are the hidden biases.\n",
        "3. The output is obtained with the weights $U$, bias $b$, and softmax activation, which maps the hidden layer to the word space probability distribution: $y=softmax(z*U+b)$.\n",
        "\n",
        "This model simultaneously learns an embedding of all the words in the\n",
        "vocabulary (embedding layer) and a model of the probability function for sequences of\n",
        "words (network output). It is able to generalize this probability function to sequences of\n",
        "words that were not seen during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ2k2KElrV-X",
        "colab_type": "text"
      },
      "source": [
        "####word2vec\n",
        "The embedding vectors are represented by the input-to-hidden weights of the network.\n",
        "They are $V * D$ shaped matrices, where $V$ is the size of the vocabulary and $D$ is the length of\n",
        "the embedding vector (which is the same as the number of neurons in the hidden layer).\n",
        "\n",
        "There are two approaches:\n",
        "* **Continuous bag of words (CBOW)**: the neural network is trained to\n",
        "predict which word fits in a sequence of words, where a single word has been\n",
        "intentionally removed, shown left in the following figure. For example, given the sqeuence \"The quick _____ fox\n",
        "jumps\", the network will predict \"brown\". But the network takes only a single word as input. Therefore, this sentence is transformed into multiple training (input, target) pairs.\n",
        "* **Skip-gram**: Given an input word, it predicts\n",
        "which words surround it, shown right in the following figure. For example, the word \"brown\" will predict the\n",
        "words \"The quick fox jumps\".\n",
        "\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/word2vec.PNG?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn7wdjEes5rL",
        "colab_type": "text"
      },
      "source": [
        "##Character-based Models for Generating New Text\n",
        "In most cases, language modeling is performed at the word level, where the distribution is over a fixed vocabulary of $|V|$ words. The character-level language models model the distribution over sequences of characters instead of words, thus allowing to compute probabilities over a much smaller vocabulary. However, by modeling the sequence of characters instead of words, much longer sequences is modeled to capture the same information over time. These long-term dependencies can be captured with LSTM language model.\n",
        "\n",
        "Because the full text is too long to train a network with **backpropagation through time (BPTT)**, a batched variant called truncated BPTT is used. The training data is divided into batches of fixed sequences length and the network is trained batch by batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1KbR53gudEu",
        "colab_type": "text"
      },
      "source": [
        "###Preprocessing and Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWsb7FDUu1CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Process text file for language model training'''\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import codecs\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdlwuWg2u82X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'war_and_peace.txt' # in\n",
        "out_file = 'wap.txt' # out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec33HlRGvDxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Regular Expression used to clean up the text\n",
        "NEW_LINE_IN_PARAGRAPH_REGEX = re.compile(r'(\\S)\\n(\\S)')\n",
        "MULTIPLE_NEWLINES_REGEX = re.compile(r'(\\n)(\\n)+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVDjj_QhvUEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read text as string\n",
        "with codecs.open(filepath, encoding='utf-8', mode='r') as f_input:\n",
        "    book_str = f_input.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bd7PLfhvdc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleanup\n",
        "book_str = NEW_LINE_IN_PARAGRAPH_REGEX.sub('\\g<1> \\g<2>', book_str)\n",
        "book_str = MULTIPLE_NEWLINES_REGEX.sub('\\n\\n', book_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmDQSSv9wPp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write processed text to file\n",
        "with codecs.open(out_file, encoding='utf-8', mode='w') as f_output:\n",
        "    f_output.write(book_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08p_-EBzw5c2",
        "colab_type": "text"
      },
      "source": [
        "To feed the data into the network, the data needs to be convertedf into a numerical format. Each character will be associated with an integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMndCjTMxCR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import codecs\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import range"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDLRDrdfxJZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataReader(object):\n",
        "    \"\"\"Data reader used for training language model.\"\"\"\n",
        "\n",
        "    def __init__(self, filepath, batch_length, batch_size):\n",
        "        self.batch_length = batch_length\n",
        "        self.batch_size = batch_size\n",
        "        # Read data into string\n",
        "        with codecs.open(filepath, encoding='utf-8', mode='r') as f:\n",
        "            self.data_str = f.read()\n",
        "        self.data_length = len(self.data_str)\n",
        "        print('data_length: ', self.data_length)\n",
        "        # Create a list of characters, indices are class indices for softmax\n",
        "        char_set = set()\n",
        "        for ch in self.data_str:\n",
        "            char_set.add(ch)\n",
        "        self.char_list = sorted(list(char_set))\n",
        "        print('char_list: ', len(self.char_list), self.char_list)\n",
        "        # Create reverse mapping to look up the index based on the character\n",
        "        self.char_dict = {val: idx for idx, val in enumerate(self.char_list)}\n",
        "        print('char_dict: ', self.char_dict)\n",
        "        # Initalise random start indices\n",
        "        self.reset_indices()\n",
        "\n",
        "    def reset_indices(self):\n",
        "        self.start_idxs = np.random.random_integers(\n",
        "            0, self.data_length, self.batch_size)\n",
        "\n",
        "    def get_sample(self, start_idx, length):\n",
        "        # Get a sample and wrap around the data string\n",
        "        return [self.char_dict[self.data_str[i % self.data_length]]\n",
        "                for i in range(start_idx, start_idx + length)]\n",
        "\n",
        "    def get_input_target_sample(self, start_idx):\n",
        "        sample = self.get_sample(start_idx, self.batch_length + 1)\n",
        "        inpt = sample[0:self.batch_length]\n",
        "        trgt = sample[1:self.batch_length + 1]\n",
        "        return inpt, trgt\n",
        "\n",
        "    def get_batch(self, start_idxs):\n",
        "        input_batch = np.zeros((self.batch_size, self.batch_length),\n",
        "                               dtype=np.int32)\n",
        "        target_batch = np.zeros((self.batch_size, self.batch_length),\n",
        "                                dtype=np.int32)\n",
        "        for i, start_idx in enumerate(start_idxs):\n",
        "            inpt, trgt = self.get_input_target_sample(start_idx)\n",
        "            input_batch[i, :] = inpt\n",
        "            target_batch[i, :] = trgt\n",
        "        return input_batch, target_batch\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            input_batch, target_batch = self.get_batch(self.start_idxs)\n",
        "            self.start_idxs = (\n",
        "                                      self.start_idxs + self.batch_length) % self.data_length\n",
        "            yield input_batch, target_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxp9_pbrxSvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = './wap.txt'\n",
        "batch_length = 10\n",
        "batch_size = 2\n",
        "reader = DataReader(filepath, batch_length, batch_size)\n",
        "s = 'As in the question of astronomy then, so in the question of history now,'\n",
        "print([reader.char_dict[c] for c in s])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUe7SuplxaAU",
        "colab_type": "text"
      },
      "source": [
        "###LSTM Network\n",
        "![](https://github.com/lblogan14/Python_Deep_Learning/blob/master/img/ch7/lstm_char_lang_model.PNG?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5K_1H9Ux2Hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import time\n",
        "\n",
        "import data_reader\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT7ANCyvx4kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "    \"\"\"RNN language model.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, sequence_length, lstm_sizes, dropout,\n",
        "                 labels, save_path):\n",
        "        self.batch_size = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.lstm_sizes = lstm_sizes\n",
        "        self.labels = labels\n",
        "        self.label_map = {val: idx for idx, val in enumerate(labels)}\n",
        "        self.number_of_characters = len(labels)\n",
        "        self.save_path = save_path\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def init_graph(self):\n",
        "        # Variable sequence length\n",
        "        self.inputs = tf.placeholder(\n",
        "            tf.int32, [self.batch_size, self.sequence_length])\n",
        "        self.targets = tf.placeholder(\n",
        "            tf.int32, [self.batch_size, self.sequence_length])\n",
        "        self.init_architecture()\n",
        "        self.saver = tf.train.Saver(tf.trainable_variables())\n",
        "\n",
        "    def init_architecture(self):\n",
        "        # Define a multilayer LSTM cell\n",
        "        self.one_hot_inputs = tf.one_hot(\n",
        "            self.inputs, depth=self.number_of_characters)\n",
        "        cell_list = [tf.nn.rnn_cell.LSTMCell(lstm_size) for lstm_size in self.lstm_sizes]\n",
        "        self.multi_cell_lstm = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
        "        # Initial state of the LSTM memory.\n",
        "        # Keep state in graph memory to use between batches\n",
        "        self.initial_state = self.multi_cell_lstm.zero_state(\n",
        "            self.batch_size, tf.float32)\n",
        "        # Convert to variables so that the state can be stored between batches\n",
        "        # Note that LSTM states is a tuple of tensors, this structure has to be\n",
        "        # re-created in order to use as LSTM state.\n",
        "        self.state_variables = tf.contrib.framework.nest.pack_sequence_as(\n",
        "            self.initial_state,\n",
        "            [tf.Variable(var, trainable=False)\n",
        "             for var in tf.contrib.framework.nest.flatten(self.initial_state)])\n",
        "        # Define the rnn through time\n",
        "        lstm_output, final_state = tf.nn.dynamic_rnn(\n",
        "            cell=self.multi_cell_lstm, inputs=self.one_hot_inputs,\n",
        "            initial_state=self.state_variables)\n",
        "        # Force the initial state to be set to the new state for the next batch\n",
        "        # before returning the output\n",
        "        store_states = [\n",
        "            state_variable.assign(new_state)\n",
        "            for (state_variable, new_state) in zip(\n",
        "                tf.contrib.framework.nest.flatten(self.state_variables),\n",
        "                tf.contrib.framework.nest.flatten(final_state))]\n",
        "        with tf.control_dependencies(store_states):\n",
        "            lstm_output = tf.identity(lstm_output)\n",
        "        # Reshape so that we can apply the linear transformation to all outputs\n",
        "        output_flat = tf.reshape(lstm_output, (-1, self.lstm_sizes[-1]))\n",
        "        # Define output layer\n",
        "        self.logit_weights = tf.Variable(\n",
        "            tf.truncated_normal(\n",
        "                (self.lstm_sizes[-1], self.number_of_characters), stddev=0.01),\n",
        "            name='logit_weights')\n",
        "        self.logit_bias = tf.Variable(\n",
        "            tf.zeros((self.number_of_characters)), name='logit_bias')\n",
        "        # Apply last layer transformation\n",
        "        self.logits_flat = tf.matmul(\n",
        "            output_flat, self.logit_weights) + self.logit_bias\n",
        "        probabilities_flat = tf.nn.softmax(self.logits_flat)\n",
        "        self.probabilities = tf.reshape(\n",
        "            probabilities_flat,\n",
        "            (self.batch_size, -1, self.number_of_characters))\n",
        "\n",
        "    def init_train_op(self, optimizer):\n",
        "        # Flatten the targets to be compatible with the flattened logits\n",
        "        targets_flat = tf.reshape(self.targets, (-1,))\n",
        "        # Get the loss over all outputs\n",
        "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits_flat, labels=targets_flat, name='x_entropy')\n",
        "        self.loss = tf.reduce_mean(loss)\n",
        "        trainable_variables = tf.trainable_variables()\n",
        "        gradients = tf.gradients(loss, trainable_variables)\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
        "        self.train_op = optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    def sample(self, session, prime_string, sample_length):\n",
        "        self.reset_state(session)\n",
        "        # Prime state\n",
        "        print('prime_string: ', prime_string)\n",
        "        for character in prime_string:\n",
        "            character_idx = self.label_map[character]\n",
        "            out = session.run(\n",
        "                self.probabilities,\n",
        "                feed_dict={self.inputs: np.asarray([[character_idx]])})\n",
        "        output_sample = prime_string\n",
        "        print('start sampling')\n",
        "        # Sample for sample_length steps\n",
        "        for _ in range(sample_length):\n",
        "            sample_label = np.random.choice(\n",
        "                self.labels, size=(1), p=out[0, 0])[0]\n",
        "            output_sample += sample_label\n",
        "            sample_idx = self.label_map[sample_label]\n",
        "            out = session.run(\n",
        "                self.probabilities,\n",
        "                feed_dict={self.inputs: np.asarray([[sample_idx]])})\n",
        "\n",
        "        return output_sample\n",
        "\n",
        "    def reset_state(self, session):\n",
        "        for state in tf.contrib.framework.nest.flatten(self.state_variables):\n",
        "            session.run(state.initializer)\n",
        "\n",
        "    def save(self, sess):\n",
        "        self.saver.save(sess, self.save_path)\n",
        "\n",
        "    def restore(self, sess):\n",
        "        self.saver.restore(sess, self.save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL21d2z8yaBy",
        "colab_type": "text"
      },
      "source": [
        "###Training and Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUsWNe3LyBQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_sample(minibatch_iterations, restore):\n",
        "    tf.reset_default_graph()\n",
        "    batch_size = 64\n",
        "    lstm_sizes = [512, 512]\n",
        "    batch_len = 100\n",
        "    learning_rate = 2e-3\n",
        "\n",
        "    filepath = './wap.txt'\n",
        "\n",
        "    data_feed = data_reader.DataReader(\n",
        "        filepath, batch_len, batch_size)\n",
        "    labels = data_feed.char_list\n",
        "    print('labels: ', labels)\n",
        "\n",
        "    save_path = './model.tf'\n",
        "    model = Model(\n",
        "        batch_size, batch_len, lstm_sizes, 0.8, labels,\n",
        "        save_path)\n",
        "    model.init_graph()\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    model.init_train_op(optimizer)\n",
        "\n",
        "    init_op = tf.initialize_all_variables()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init_op)\n",
        "        if restore:\n",
        "            print('Restoring model')\n",
        "            model.restore(sess)\n",
        "        model.reset_state(sess)\n",
        "        start_time = time.time()\n",
        "        for i in range(minibatch_iterations):\n",
        "            input_batch, target_batch = next(iter(data_feed))\n",
        "            loss, _ = sess.run(\n",
        "                [model.loss, model.train_op],\n",
        "                feed_dict={model.inputs: input_batch, model.targets: target_batch})\n",
        "            if i % 50 == 0 and i != 0:\n",
        "                print('i: ', i)\n",
        "                duration = time.time() - start_time\n",
        "                print('loss: {} ({} sec.)'.format(loss, duration))\n",
        "                start_time = time.time()\n",
        "            if i % 1000 == 0 and i != 0:\n",
        "                model.save(sess)\n",
        "            if i % 100 == 0 and i != 0:\n",
        "                print('Reset initial state')\n",
        "                model.reset_state(sess)\n",
        "            if i % 1000 == 0 and i != 0:\n",
        "                print('Reset minibatch feeder')\n",
        "                data_feed.reset_indices()\n",
        "        model.save(sess)\n",
        "\n",
        "    print('\\n sampling after {} iterations'.format(minibatch_iterations))\n",
        "    tf.reset_default_graph()\n",
        "    model = Model(\n",
        "        1, None, lstm_sizes, 1.0, labels, save_path)\n",
        "    model.init_graph()\n",
        "    init_op = tf.initialize_all_variables()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init_op)\n",
        "        model.restore(sess)\n",
        "        print('\\nSample 1:')\n",
        "        sample = model.sample(\n",
        "            sess, prime_string=u'\\n\\nThis feeling was ', sample_length=500)\n",
        "        print(u'sample: \\n{}'.format(sample))\n",
        "        print('\\nSample 2:')\n",
        "        sample = model.sample(\n",
        "            sess, prime_string=u'She was born in the year ', sample_length=500)\n",
        "        print(u'sample: \\n{}'.format(sample))\n",
        "        print('\\nSample 3:')\n",
        "        sample = model.sample(\n",
        "            sess, prime_string=u'The meaning of this all is ',\n",
        "            sample_length=500)\n",
        "        print(u'sample: \\n{}'.format(sample))\n",
        "        print('\\nSample 4:')\n",
        "        sample = model.sample(\n",
        "            sess,\n",
        "            prime_string=u'In the midst of a conversation on political matters Anna Pvlovna burst out:,',\n",
        "            sample_length=500)\n",
        "        print(u'sample: \\n{}'.format(sample))\n",
        "        print('\\nSample 5:')\n",
        "        sample = model.sample(\n",
        "            sess, prime_string=u'\\n\\nCHAPTER X\\n\\n',\n",
        "            sample_length=500)\n",
        "        print(u'sample: \\n{}'.format(sample))\n",
        "        print('\\nSample 5:')\n",
        "        sample = model.sample(\n",
        "            sess, prime_string=u'\"If only you knew,\"',\n",
        "            sample_length=500)\n",
        "        print(u'sample: \\n{}'.format(sample))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzj58D2dyGlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_iterations = 500\n",
        "print('\\n\\n\\nTrain for {}'.format(500))\n",
        "print('Total iters: {}'.format(total_iterations))\n",
        "train_and_sample(500, restore=False)\n",
        "for i in [500, 1000, 3000, 5000, 10000, 30000, 50000, 100000, 300000]:\n",
        "    total_iterations += i\n",
        "    print('\\n\\n\\nTrain for {}'.format(i))\n",
        "    print('Total iters: {}'.format(total_iterations))\n",
        "    train_and_sample(i, restore=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}